# Tryout LocalElasticity and Overparameterization with Neurashed

# Table of Contents
1. [Prior Sequestial Work](#prior-sequential-work)
2. [Additional Useful Repositories](#additional-useful-repositories)

# Prior Sequential Work
| Title | Description | Code |
|-------|-------------|------|
|  [Imitating Deep Learning Dynamics via Locally Elastic Stochastic Differential Equations](https://proceedings.neurips.cc/paper/2021/file/327af0f71f7acdfd882774225f04775f-Paper.pdf) <br> Jiayao Zhang, Hua Wang, Weijie Su     |    <li>Model the evolution of features during deep learning training using a set of stochastic differential equations (SDEs) that each corresponds to a training sampl</li> <li>If the SDEs are locally elastic in the sense that the impact is more significant on samples from the same class as the input, the features of the training data become linearly separable, meaning vanishing training loss; otherwise, the features are not separable, regardless of how long the training time is</li> <li>Show that the emergence of a simple geometric structure called the neural collapse of the features</li>   |   [git](https://github.com/zjiayao/le_sde)   |

# Additional Useful Repositories
* [Exploring Generalization in Deep Learning](https://github.com/bneyshabur/generalization-bounds)
